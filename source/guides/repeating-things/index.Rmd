---
layout: page
title: "Repeating things: looping and the apply family"
date: 2013-03-18 10:25
comments: true
sharing: true
footer: false
---

Previously we looked at how you can [use functions to simplify your
code](../functions/). Ideally you have a function that performs a single
operation, and now you want to use it many times to do the same operation on
lots of different data. The naive way to do that would be something like this:

```{r, eval=FALSE}
  res1 <-  f(input1)
  res2 <-  f(input2)
  ...
  res10 <-  f(input10)
```

But this isn't very *nice*. Yes, by using a function, you have reduced a
substantial amount of repetition. That IS nice. But there is still repetition.
Repeating yourself will cost you time, both now and later, and potentially
introduce some nasty  bugs. When it comes to repetition, well, just don't.

The nice way of repeating elements of code is to use a loop of some sort. A
loop is a coding structure that reruns the same bit of code over and  over, but
with only small fragments differing between runs. In R there  is a whole family
of looping functions, each with their own strengths.

# The split--apply--combine pattern

First, it is good to recognise that most operations that involve looping are
instances of the *split apply combine* strategy. You start with a bunch of data.
Then you then **Split** it up into many smaller datasets, **Apply** a function
to each piece, and finally **Combine** the results back together.

Some data arrives already in its pieces - e.g. output files from from a leaf
scanner or temperature machine. Your job is then to analyse each bit, and put
them together into a larger data set.

Sometimes the combine phase means making a new data frame, other times it might
mean something more abstract, like combining a bunch of plots in a report.

Either way, the challenge for you is to identify the pieces that remain the same
between different runs of your function, then structure your analysis around
that.

# Why we're not starting with `for` loops

Ok, you got me, we are starting with for loops. But not in the way you think.

When you mention looping, many people immediately reach for `for`. Perhaps
that's because, like me, they are already familiar with these other languages,
like basic, python, perl, C, C++ or matlab. While `for` is definitely the most
flexible of the looping options, we suggest you avoid it wherever you can, for
the following two reasons:

1. It is not very expressive, i.e. takes a lot of code to do what you want.
2. It permits you to write horrible code, like this example from my earlier
   work:

```{r, eval=FALSE}
  for(n in 1:n.spp)
    {
    Ind = unique(Raw[Raw$SPP==as.character(sp.list$SPP[n]), "INDIV"]);
    I =    length(Ind);
    par(mfrow=c(I,1), oma=c(5,5,5, 2), mar=c(3, 0, 0, 0));
    for(i in 1:I)
        {
        Dat = subset(Raw, Raw$SPP==as.character(sp.list$SPP[n]) & Raw$INDIV==Ind[i])
        Y_ax =seq(0, 35, 10); Y_ax2=seq(0, 35, 5);
        X_ax =seq(0, max(Dat$LL), 0.2); X_ax2 =seq(0, max(Dat$LL), 0.1);
        plot(1:2, 1:2, type="n",log="", axes=F,ann=F, xlim = c(-0.05, max(Dat$LL)+0.05), ylim=c(min(Y_ax), max(Y_ax)), xaxs="i", yaxs="i", las=1)
        axis(2, at=Y_ax, labels=Y_ax, las=1, tck=0.030, cex.axis=0.8, adj = 0.5)
        axis(4, at=Y_ax, labels=F,  tck=0.03)

        X<-Dat$AGE;  Xout<-data.frame(X = c(0,Dat$LL[1]))

        Y<-Dat$S2AV_L;
        points(X,Y,type="p", pch=Fig$Symb[2], cex=1.3, col= Fig$Cols[2]);
        R<-lm( Y~ X); points(Xout$X, predict(R, Xout), type="l", col= Fig$Cols[2], lty = "dotted")

        legend("topright", legend = paste("indiv = ",Ind[i]), bty= "n")
       }
   mtext(expression(paste("Intercepted light (mol ", m^{-2},d^{-1},")")), side = 2, line = 3, outer = T, adj = 0.5, cex =1.2)
   mtext(expression(paste("Leaf age (yrs)")), side = 1, line = 0.2, outer = T, adj = 0.5, cex =1.2)
   mtext(as.character(sp.list$Species[n]), side = 3, line = 2, outer = T, adj = 0.5, cex =1.5)
   }
rm(R, Ind, I, i, X, X_ax, X_ax2, Y_ax, Y_ax2, Y, Xout, Dat)
```

The main problems with this code are that

- it is hard to read
- all the variables are stored in the global scope, which is dangerous.

All it's doing is making a plot! Compare that to something like this

```{r, eval=FALSE}
  lapply(unique(Raw$SPP), makePlot, data = Raw)
```
That's much nicer! It's obvious what the loop does, and no new variables are
created. Of course, for the code to work, we need to define the function

```{r, eval=FALSE}
makePlot <- function(species, data){
  ... #do stuff
}
```

which actually makes our plot, but having all that detail off in a function has
many benefits. Most of all it makes your code more reliable and easier to read.

So our reason for avoiding `for` loops, and the similar functions `while` and `repeat`,
is that the other looping functions, like
`lapply`, demand that you write nicer code, so that's we'll focus on first.

# The apply family

There are several related function in R which allow you to apply some function
to a series of objects (eg. vectors, matrices, dataframes or files). They include:

- lapply
- sapply
- tapply
- aggregate
- mapply
- apply.

Each repeats a function or operation on a series of elements, but they differ in
the data types they accept and return. What they all in common is that **order
of operation is not important**.  This is crucial. If each each operation is
independent, then you can run them in whatever order you like. Generally, we
argue that you should only use the generic looping functions `for`,
`while`, and `repeat` when the order or operations *IS* important. Otherwise
reach for one of the apply tools.

# lapply

`lapply` applies a function to each element of a list (or vector), collecting
results in a list.

Lists are a very powerful and flexible data structure that few people seem to
know about. Moreover, they are the building block for other data structures,
like `data.frame` and `matrix`. To access elements of a list, you use the
double square bracket, for example `X[[4]]` returns the fourth element of the
list `X`. If you don't know what a list is, we suggest you [read more
about them](http://cran.r-project.org/doc/manuals/R-intro.html#Lists-and-
data-frames), before you proceed.

## Basic syntax

```{r, eval=FALSE}
result <- lapply(X, f, ...)
```

Here `X` is a list or vector, containing the elements that form the input to the
function `f`. This code will also return a list, stored in `result`, with same
number of elements as `X`.

## Usage

lapply is great for building analysis pipelines, where you want to repeat a
series of steps on a large number of similar objects.  The way to do this is to
have a series of lapply statements, with the output of one providing the input to
another:

```{r, eval=FALSE}
first.step <- lapply(X, first.function)
second.step <- lapply(first.step, next.function)
```

The challenge is to identify the parts of your analysis that stay the same and
those that differ for each call of the function. The trick to using `lapply` is
to recognise that only one item can differ between different function calls.

It is possible to pass in a bunch of additional arguments to your function, but
these must be the same for each call of your function. For example, let's say we
have a function `test` which takes the path of a file, loads the data, and tests
it against some hypothesised value H0. We can run the function on the file
"myfile.csv" as follows.

```{r, eval=FALSE}
result <- test("myfile.csv", H0=1)
```

We could then run the test on a bunch of files using lapply:
```{r, eval=FALSE}
files <- c("myfile1.csv", "myfile2.csv", "myfile3.csv")
result <- lapply(files, test, H0=1)
```

But notice, that in this example, the **only this that differs** between the runs
is a single number in the file name. So we could save ourselves typing these by
adding an extra step to generate the file names

```{r, eval=FALSE}
files <- lapply(1:10, function(x){paste0("myfile", x, ".csv")})
result <- lapply(files, test, H0=1)
```

The nice things about that piece of code is that it would extend as long as we
wanted, to 10000000 files, if needed.

## Example - plotting temperature for many sites using open weather data

The following code provides an example of an analysis pipeline.


## Tricks

### Combining output

TODO (RF): use do.call(rbind) to combining together

Use sapply:  - sapply --> vector, matrix


### Parallelising your code

Another great feature of lapply is that is **makes it really easy to parallelise
your code**. All computers now contain multiple CPUs, and these can all be put to
work using the great [multicore package](http://www.rforge.net/multicore/).

```{r, eval=FALSE}
result <- lapply(x, f)   #apply f to x using a single core and lapply

library(multicore)
result <- mclapply(x, f) #same thing using all the cores in your machine
```

# sapply, tapply and aggregate

As we saw above, lists can be very useful, but if you don't want output as a
list, they can be a little awkward to work with. `sapply`, `tapply` and
`aggregate` provide other ways of repeating things, with more customised
output.

We're going to illustrate the strengths of each, with an example. For that, we
need some data. We're going to use some [data on seinfeld episodes]
(https://github.com/audy/smalldata), taken from the [Internet movie Data Base]
(http://www.reddit.com/r/dataisbeautiful/comments/1g7jw2/seinfeld_imdb_episode_ratings_oc/).

``` {r }
library(downloader)
download("https://raw.github.com/audy/smalldata/master/seinfeld.csv",
         "seinfeld.csv")
dat <- read.csv("seinfeld.csv", stringsAsFactors=FALSE)

```
Columns are Season (number), Episode (number), Title (of the
episode), Rating (according to IMDb) and Votes (to construct the
rating).

``` {r }
head(dat)
```

Make sure it's sorted sensibly
``` {r }
dat <- dat[order(dat$Season, dat$Episode),]
```

Biologically, this could be Site / Individual / ID / Mean size /
Things measured.

Hypothesis: Seinfeld used to be funny, but got progressively less
good as it became too mainstream.  Or, does the mean episode rating
per season decrease?

Now, we want to calculate the average rating per season:
``` {r }
mean(dat$Rating[dat$Season == 1])
mean(dat$Rating[dat$Season == 2])
```
and so on until:
``` {r }
mean(dat$Rating[dat$Season == 9])
```

As with most things, we *could* automate this with a for loop:

``` {r }
seasons <- sort(unique(dat$Season))
rating  <- numeric(length(seasons))
for (i in seq_along(seasons))
  rating[i] <- mean(dat$Rating[dat$Season == seasons[i]])
```

That's actually not that horrible to do.  But we it could be
nicer.  We first *split* the ratings by season:
``` {r }
ratings.split <- split(dat$Rating, dat$Season)
head(ratings.split)
```

Then use sapply to loop over this list, computing the mean
``` {r }
rating <- sapply(ratings.split, mean)
```

Then if we wanted to apply a different function (say, compute the
per-season standard error) we could just do:
``` {r }
se <- function(x)
  sqrt(var(x) / length(x))
rating.se <- sapply(ratings.split, se)

plot(rating ~ seasons, ylim=c(7, 9), pch=19)
arrows(seasons, rating - rating.se, seasons, rating + rating.se,
       code=3, angle=90, length=0.02)
```

But there's still repetition there.  Let's abstract that away a bit.

Suppose we want a:
  1. response variable (like Rating was)
  2. grouping variable (like Season was)
  3. function to apply to each level

This just writes out *exactly* what we had before
``` {r }
summarise.by.group <- function(response, group, func) {
  response.split <- split(response, group)
  sapply(response.split, func)
}
```

We can compute the mean rating by season again:
``` {r }
rating.new <- summarise.by.group(dat$Rating, dat$Season, mean)
```

which is the same as what we got before:
``` {r }
identical(rating.new, rating)
```

Of course, we're not the first people to try this.  This is EXACTLY
what the `tapply` function does (but with a few bells and whistles,
especially around missing values, factor levels, additional
arguments and multiple grouping factors at once).
``` {r }
tapply(dat$Rating, dat$Season, mean)
```
So using tapply, you can do all the above manipulation in a
single line.

The first is that getting the season out of `tapply` is quite
hard.  We could do:
``` {r }
as.numeric(names(rating))
```

But that's quite ugly, not least because it involves the conversion
numeric -> string -> numeric.

Better could be to use
``` {r }
sort(unique(dat$Season))
```

But that requires knowing what is going on inside of `tapply`.

I suspect that this approach:
``` {r }
first <- function(x) x[[1]]
tapply(dat$Season, dat$Season, first)
```
is probably the most fool-proof, but it's certainly not pretty.


The `aggregate` function provides a simplfied interface to `tapply`
that avoids this issue.  It has two interfaces: the first is
similar to what we used before, but the grouping variable now must
be a list or data frame:
``` {r }
aggregate(dat$Rating, dat["Season"], mean)
```

(note that `dat["Season"]` returns a one-column data frame).  The
column 'x' is our response variable, Rating, grouped by season.  We
can get its name included in the column names here by specifying
the first argument as a `data.frame` too:
``` {r }
aggregate(dat["Rating"], dat["Season"], mean)
```

The other interface is the formula interface, that will be familiar
from fitting linear models:
``` {r }
aggregate(Rating ~ Season, dat, mean)
```

This interface is really nice; we can get the number of votes here
too.
``` {r }
aggregate(cbind(Rating, Votes) ~ Season, dat, mean)
```

If you have multiple grouping variables, you can write things like:
```
aggregate(response ~ factor1 + factor2, dat, function)
```
to apply a function to each pair of levels of factor1 and factor2.

# replicate

This is great in Monte Carlo simulation situations.  For example.
Suppose that you flip a fair coin n times and count the number of
heads:

``` {r }
trial <- function(n)
  sum(runif(n) < 0.5) # could have done a binomial draw...
```

You can run the trial a bunch of times:
``` {r }
trial(10)
trial(10)
trial(10)
```

and get a feel for the results.  If you want to replicate the trial
100 times and look at the distribution of results, you could do:
``` {r }
replicate(100, trial(10))
```

and then you could plot these:
``` {r }
plot(table(replicate(10000, trial(50))))
```

# for loops

"`for`" loops shine where the output of one iteration depends on
the result of the previous iteration.

Suppose you wanted to model random walk.  Every time step, with 50%
probability move left or right.

Start at position 0
``` {r }
x <- 0
```
Move left or right with probability p (0.5 = unbiased)
``` {r }
p <- 0.5
```

Update the position
``` {r }
x <- x + if (runif(1) < p) -1 else 1
```

Let's abstract the update into a function:
``` {r }
step <- function(x, p=0.5)
  x + if (runif(1) < p) -1 else 1
```

Repeat a bunch of times:
``` {r }
x <- step(x)
x <- step(x)
```

To find out where we got to after 20 steps:
``` {r }
for (i in 1:20)
  x <- step(x)
```

If we want to collect where we're up to at the same time:
``` {r }
nsteps <- 200
x <- numeric(nsteps + 1)
x[1] <- 0 # start at 0
for (i in seq_len(nsteps))
  x[i+1] <- step(x[i])
plot(x, type="l")
```

Pulling *that* into a function:
``` {r }
random.walk <- function(nsteps, x0=0, p=0.5) {
  x <- numeric(nsteps + 1)
  x[1] <- x0
  for (i in seq_len(nsteps))
    x[i+1] <- step(x[i])
  x
}
```

We can then do 30 random walks:
``` {r }
walks <- replicate(30, random.walk(100))
matplot(walks, type="l", lty=1, col=rainbow(nrow(walks)))
```

Of course, in this case, if we think in terms of vectors we can
actually implement random walk using implicit vectorisation:
``` {r }
random.walk <- function(nsteps, x0=0, p=0.5)
  cumsum(c(x0, ifelse(runif(nsteps) < p, -1, 1)))

walks <- replicate(30, random.walk(100))
matplot(walks, type="l", lty=1, col=rainbow(nrow(walks)))
```

Which reinforces one of the advantages of thinking in terms of
functions: you can change the implementation detail without the
rest of the program changing.



